{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[{"file_id":"1J7CGQhgyjIgNHl5aMEcyWxsLoWW3ujBZ","timestamp":1729517578809},{"file_id":"1MZXuLfVsmpeAXFSjVJDPLzl5v6tq6h-U","timestamp":1729423632161}],"gpuType":"T4","authorship_tag":"ABX9TyPU13pjelZd6EJZBU1KGOwH"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"code","source":["# バージョンの問題でエラーが出たため\n","!pip install pip==23.0.1"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"kZQ6eMpy3MsF","executionInfo":{"status":"ok","timestamp":1729753243496,"user_tz":-540,"elapsed":9648,"user":{"displayName":"shuto u","userId":"05197020628006578565"}},"outputId":"3417f806-a117-48d6-96d4-a59e985e71a0"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting pip==23.0.1\n","  Downloading pip-23.0.1-py3-none-any.whl.metadata (4.1 kB)\n","Downloading pip-23.0.1-py3-none-any.whl (2.1 MB)\n","\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/2.1 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.3/2.1 MB\u001b[0m \u001b[31m10.0 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m2.0/2.1 MB\u001b[0m \u001b[31m29.3 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.1/2.1 MB\u001b[0m \u001b[31m20.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hInstalling collected packages: pip\n","  Attempting uninstall: pip\n","    Found existing installation: pip 24.1.2\n","    Uninstalling pip-24.1.2:\n","      Successfully uninstalled pip-24.1.2\n","Successfully installed pip-23.0.1\n"]}]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Cys9Tf8X2va-","executionInfo":{"status":"ok","timestamp":1729476901604,"user_tz":-540,"elapsed":56894,"user":{"displayName":"shuto u","userId":"05197020628006578565"}},"outputId":"a4819898-54b6-4ded-cd93-a05070467011"},"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting fairseq\n","  Downloading fairseq-0.12.2.tar.gz (9.6 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m9.6/9.6 MB\u001b[0m \u001b[31m20.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n","  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n","  Installing backend dependencies ... \u001b[?25l\u001b[?25hdone\n","  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n","Collecting sacrebleu>=1.4.12\n","  Downloading sacrebleu-2.4.3-py3-none-any.whl (103 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m104.0/104.0 kB\u001b[0m \u001b[31m14.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting hydra-core<1.1,>=1.0.7\n","  Downloading hydra_core-1.0.7-py3-none-any.whl (123 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m123.8/123.8 kB\u001b[0m \u001b[31m16.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: torchaudio>=0.8.0 in /usr/local/lib/python3.10/dist-packages (from fairseq) (2.4.1+cu121)\n","Requirement already satisfied: regex in /usr/local/lib/python3.10/dist-packages (from fairseq) (2024.9.11)\n","Collecting omegaconf<2.1\n","  Downloading omegaconf-2.0.6-py3-none-any.whl (36 kB)\n","Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (from fairseq) (2.4.1+cu121)\n","Collecting bitarray\n","  Downloading bitarray-3.0.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (278 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m278.3/278.3 kB\u001b[0m \u001b[31m32.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from fairseq) (4.66.5)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from fairseq) (1.26.4)\n","Requirement already satisfied: cffi in /usr/local/lib/python3.10/dist-packages (from fairseq) (1.17.1)\n","Requirement already satisfied: cython in /usr/local/lib/python3.10/dist-packages (from fairseq) (3.0.11)\n","Collecting antlr4-python3-runtime==4.8\n","  Downloading antlr4-python3-runtime-4.8.tar.gz (112 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m112.4/112.4 kB\u001b[0m \u001b[31m16.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","Requirement already satisfied: PyYAML>=5.1.* in /usr/local/lib/python3.10/dist-packages (from omegaconf<2.1->fairseq) (6.0.2)\n","Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from omegaconf<2.1->fairseq) (4.12.2)\n","Collecting colorama\n","  Downloading colorama-0.4.6-py2.py3-none-any.whl (25 kB)\n","Requirement already satisfied: lxml in /usr/local/lib/python3.10/dist-packages (from sacrebleu>=1.4.12->fairseq) (4.9.4)\n","Collecting portalocker\n","  Downloading portalocker-2.10.1-py3-none-any.whl (18 kB)\n","Requirement already satisfied: tabulate>=0.8.9 in /usr/local/lib/python3.10/dist-packages (from sacrebleu>=1.4.12->fairseq) (0.9.0)\n","Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch->fairseq) (1.13.3)\n","Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch->fairseq) (3.4.1)\n","Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch->fairseq) (2024.6.1)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch->fairseq) (3.1.4)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch->fairseq) (3.16.1)\n","Requirement already satisfied: pycparser in /usr/local/lib/python3.10/dist-packages (from cffi->fairseq) (2.22)\n","Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch->fairseq) (3.0.1)\n","Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch->fairseq) (1.3.0)\n","Building wheels for collected packages: fairseq, antlr4-python3-runtime\n","  Building wheel for fairseq (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for fairseq: filename=fairseq-0.12.2-cp310-cp310-linux_x86_64.whl size=11288612 sha256=db70eaa5666d20273e0c478226c9712634e2f5f3d7556ca5632f2217edf377ef\n","  Stored in directory: /root/.cache/pip/wheels/e4/35/55/9c66f65ec7c83fd6fbc2b9502a0ac81b2448a1196159dacc32\n","  Building wheel for antlr4-python3-runtime (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for antlr4-python3-runtime: filename=antlr4_python3_runtime-4.8-py3-none-any.whl size=141214 sha256=fe5796c7df20a64080116613db1486718eaf96a04c99edcac8412d2ee276bd6a\n","  Stored in directory: /root/.cache/pip/wheels/a7/20/bd/e1477d664f22d99989fd28ee1a43d6633dddb5cb9e801350d5\n","Successfully built fairseq antlr4-python3-runtime\n","Installing collected packages: bitarray, antlr4-python3-runtime, portalocker, omegaconf, colorama, sacrebleu, hydra-core, fairseq\n","Successfully installed antlr4-python3-runtime-4.8 bitarray-3.0.0 colorama-0.4.6 fairseq-0.12.2 hydra-core-1.0.7 omegaconf-2.0.6 portalocker-2.10.1 sacrebleu-2.4.3\n"]}],"source":["# 90\n","# fairseqのインストール\n","!pip install fairseq"]},{"cell_type":"code","source":["# データセットのダウンロード\n","!wget http://www.phontron.com/kftt/download/kftt-data-1.0.tar.gz\n","\n","# データセットの解凍\n","!tar -zxvf kftt-data-1.0.tar.gz"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"u_OTbI-03rjl","executionInfo":{"status":"ok","timestamp":1729476907244,"user_tz":-540,"elapsed":5643,"user":{"displayName":"shuto u","userId":"05197020628006578565"}},"outputId":"344fbed5-c213-4139-b2d9-3289ad1987eb"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["--2024-10-21 02:15:01--  http://www.phontron.com/kftt/download/kftt-data-1.0.tar.gz\n","Resolving www.phontron.com (www.phontron.com)... 173.236.247.185\n","Connecting to www.phontron.com (www.phontron.com)|173.236.247.185|:80... connected.\n","HTTP request sent, awaiting response... 200 OK\n","Length: 99246893 (95M) [application/gzip]\n","Saving to: ‘kftt-data-1.0.tar.gz’\n","\n","kftt-data-1.0.tar.g 100%[===================>]  94.65M  40.0MB/s    in 2.4s    \n","\n","2024-10-21 02:15:03 (40.0 MB/s) - ‘kftt-data-1.0.tar.gz’ saved [99246893/99246893]\n","\n","kftt-data-1.0/\n","kftt-data-1.0/data/\n","kftt-data-1.0/data/orig/\n","kftt-data-1.0/data/orig/kyoto-tune.en\n","kftt-data-1.0/data/orig/kyoto-dev.ja\n","kftt-data-1.0/data/orig/kyoto-dev.en\n","kftt-data-1.0/data/orig/kyoto-train.en\n","kftt-data-1.0/data/orig/kyoto-tune.ja\n","kftt-data-1.0/data/orig/kyoto-train.ja\n","kftt-data-1.0/data/orig/kyoto-test.ja\n","kftt-data-1.0/data/orig/kyoto-test.en\n","kftt-data-1.0/data/tok/\n","kftt-data-1.0/data/tok/kyoto-tune.en\n","kftt-data-1.0/data/tok/kyoto-dev.ja\n","kftt-data-1.0/data/tok/kyoto-train.cln.en\n","kftt-data-1.0/data/tok/kyoto-dev.en\n","kftt-data-1.0/data/tok/kyoto-train.en\n","kftt-data-1.0/data/tok/kyoto-tune.ja\n","kftt-data-1.0/data/tok/kyoto-train.cln.ja\n","kftt-data-1.0/data/tok/kyoto-train.ja\n","kftt-data-1.0/data/tok/kyoto-test.ja\n","kftt-data-1.0/data/tok/kyoto-test.en\n","kftt-data-1.0/README.txt\n"]}]},{"cell_type":"code","source":["# トークナイズされた英語と日本語のデータを確認\n","!head kftt-data-1.0/data/tok/kyoto-train.en\n","!head kftt-data-1.0/data/tok/kyoto-train.ja"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"AGECRVi9348r","executionInfo":{"status":"ok","timestamp":1729476907245,"user_tz":-540,"elapsed":8,"user":{"displayName":"shuto u","userId":"05197020628006578565"}},"outputId":"7b4603ed-286d-42ba-d5b4-0c966d974e5a"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Known as Sesshu ( 1420 - 1506 ) , he was an ink painter and Zen monk active in the Muromachi period in the latter half of the 15th century , and was called a master painter .\n","He revolutionized the Japanese ink painting .\n","He was given the posthumous name \" Toyo \" or \" Sesshu ( 拙宗 ) . \"\n","Born in Bicchu Province , he moved to Suo Province after entering SShokoku-ji Temple in Kyoto .\n","Later he accompanied a mission to Ming Dynasty China and learned Chinese ink painting .\n","His works were many , including not only Chinese-style landscape paintings , but also portraits and pictures of flowers and birds .\n","His bold compositions and strong brush strokes constituted an extremely distinctive style .\n","6 of his extant works are designated national treasures . Indeed , he is considered to be extraordinary among Japanese painters .\n","For this reason , there are a great many artworks that are attributed to him , such as folding screens with pictures of flowers and that birds are painted on them .\n","There are many works that even experts cannot agree if they are really his work or not .\n","雪舟 （ せっしゅう 、 1420 年 （ 応永 27 年 ） - 1506 年 （ 永正 3 年 ） ） は 号 で 、 15 世紀 後半 室町 時代 に 活躍 し た 水墨 画 家 ・ 禅僧 で 、 画聖 と も 称え られ る 。\n","日本 の 水墨 画 を 一変 さ せ た 。\n","諱 は 「 等楊 （ とうよう ） 」 、 もしくは 「 拙宗 （ せっしゅう ） 」 と 号 し た 。\n","備中 国 に 生まれ 、 京都 ・ 相国 寺 に 入 っ て から 周防 国 に 移 る 。\n","その 後 遣明 使 に 随行 し て 中国 （ 明 ） に 渡 っ て 中国 の 水墨 画 を 学 ん だ 。\n","作品 は 数 多 く 、 中国 風 の 山水 画 だけ で な く 人物 画 や 花鳥 画 も よ く し た 。\n","大胆 な 構図 と 力強 い 筆線 は 非常 に 個性 的 な 画風 を 作り出 し て い る 。\n","現存 する 作品 の うち 6 点 が 国宝 に 指定 さ れ て お り 、 日本 の 画家 の なか で も 別格 の 評価 を 受け て い る と いえ る 。\n","この ため 、 花鳥 図 屏風 など に 「 伝 雪舟 筆 」 さ れ る 作品 は 大変 多 い 。\n","真筆 で あ る か 専門 家 の 間 で も 意見 の 分かれ る もの も 多々 あ る 。\n"]}]},{"cell_type":"code","source":["# Fairseqの前処理とバイナリデータセットを作成\n","!fairseq-preprocess \\\n","    --source-lang ja --target-lang en \\\n","    --trainpref kftt-data-1.0/data/tok/kyoto-train \\\n","    --validpref kftt-data-1.0/data/tok/kyoto-dev \\\n","    --testpref kftt-data-1.0/data/tok/kyoto-test \\\n","    --destdir data-bin/kftt \\\n","    --workers 2"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Wn5I94VL38k7","executionInfo":{"status":"ok","timestamp":1729477184022,"user_tz":-540,"elapsed":276782,"user":{"displayName":"shuto u","userId":"05197020628006578565"}},"outputId":"99e8dc0b-d3f6-4ef6-bd32-b07fcf32200d"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["2024-10-21 02:15:11.608527: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n","2024-10-21 02:15:11.629490: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n","2024-10-21 02:15:11.635472: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","2024-10-21 02:15:11.649332: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n","To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n","2024-10-21 02:15:12.985152: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n","2024-10-21 02:15:19 | INFO | fairseq.tasks.text_to_speech | Please install tensorboardX: pip install tensorboardX\n","2024-10-21 02:15:19 | INFO | fairseq_cli.preprocess | Namespace(no_progress_bar=False, log_interval=100, log_format=None, log_file=None, aim_repo=None, aim_run_hash=None, tensorboard_logdir=None, wandb_project=None, azureml_logging=False, seed=1, cpu=False, tpu=False, bf16=False, memory_efficient_bf16=False, fp16=False, memory_efficient_fp16=False, fp16_no_flatten_grads=False, fp16_init_scale=128, fp16_scale_window=None, fp16_scale_tolerance=0.0, on_cpu_convert_precision=False, min_loss_scale=0.0001, threshold_loss_scale=None, amp=False, amp_batch_retries=2, amp_init_scale=128, amp_scale_window=None, user_dir=None, empty_cache_freq=0, all_gather_list_size=16384, model_parallel_size=1, quantization_config_path=None, profile=False, reset_logging=False, suppress_crashes=False, use_plasma_view=False, plasma_path='/tmp/plasma', criterion='cross_entropy', tokenizer=None, bpe=None, optimizer=None, lr_scheduler='fixed', scoring='bleu', task='translation', source_lang='ja', target_lang='en', trainpref='kftt-data-1.0/data/tok/kyoto-train', validpref='kftt-data-1.0/data/tok/kyoto-dev', testpref='kftt-data-1.0/data/tok/kyoto-test', align_suffix=None, destdir='data-bin/kftt', thresholdtgt=0, thresholdsrc=0, tgtdict=None, srcdict=None, nwordstgt=-1, nwordssrc=-1, alignfile=None, dataset_impl='mmap', joined_dictionary=False, only_source=False, padding_factor=8, workers=2, dict_only=False)\n","2024-10-21 02:16:19 | INFO | fairseq_cli.preprocess | [ja] Dictionary: 146832 types\n","2024-10-21 02:18:12 | INFO | fairseq_cli.preprocess | [ja] kftt-data-1.0/data/tok/kyoto-train.ja: 440288 sents, 12359715 tokens, 0.0% replaced (by <unk>)\n","2024-10-21 02:18:12 | INFO | fairseq_cli.preprocess | [ja] Dictionary: 146832 types\n","2024-10-21 02:18:12 | INFO | fairseq_cli.preprocess | [ja] kftt-data-1.0/data/tok/kyoto-dev.ja: 1166 sents, 28010 tokens, 0.518% replaced (by <unk>)\n","2024-10-21 02:18:12 | INFO | fairseq_cli.preprocess | [ja] Dictionary: 146832 types\n","2024-10-21 02:18:13 | INFO | fairseq_cli.preprocess | [ja] kftt-data-1.0/data/tok/kyoto-test.ja: 1160 sents, 29638 tokens, 0.55% replaced (by <unk>)\n","2024-10-21 02:18:13 | INFO | fairseq_cli.preprocess | [en] Dictionary: 221864 types\n","2024-10-21 02:19:40 | INFO | fairseq_cli.preprocess | [en] kftt-data-1.0/data/tok/kyoto-train.en: 440288 sents, 11981667 tokens, 0.0% replaced (by <unk>)\n","2024-10-21 02:19:40 | INFO | fairseq_cli.preprocess | [en] Dictionary: 221864 types\n","2024-10-21 02:19:40 | INFO | fairseq_cli.preprocess | [en] kftt-data-1.0/data/tok/kyoto-dev.en: 1166 sents, 25475 tokens, 1.79% replaced (by <unk>)\n","2024-10-21 02:19:40 | INFO | fairseq_cli.preprocess | [en] Dictionary: 221864 types\n","2024-10-21 02:19:41 | INFO | fairseq_cli.preprocess | [en] kftt-data-1.0/data/tok/kyoto-test.en: 1160 sents, 27894 tokens, 1.55% replaced (by <unk>)\n","2024-10-21 02:19:41 | INFO | fairseq_cli.preprocess | Wrote preprocessed data to data-bin/kftt\n"]}]},{"cell_type":"code","source":["# 91\n","# LSTMを使って翻訳モデルをトレーニング\n","!fairseq-train data-bin/kftt \\\n","    --arch lstm --share-decoder-input-output-embed \\\n","    --optimizer adam --lr 0.001 --clip-norm 0.1 \\\n","    --lr-scheduler inverse_sqrt --warmup-updates 4000 \\\n","    --dropout 0.3 --max-tokens 4096 --fp16 \\\n","    --save-dir checkpoints/kftt-checkpoints \\\n","    --max-epoch 7"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"2QeWA1Dc3_xR","executionInfo":{"status":"ok","timestamp":1729486633789,"user_tz":-540,"elapsed":9386458,"user":{"displayName":"shuto u","userId":"05197020628006578565"}},"outputId":"a6527147-0917-4c66-a6fb-189b530517fc"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["2024-10-21 02:20:49.202886: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n","2024-10-21 02:20:49.222304: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n","2024-10-21 02:20:49.228255: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","2024-10-21 02:20:49.242120: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n","To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n","2024-10-21 02:20:50.298348: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n","2024-10-21 02:20:51 | INFO | numexpr.utils | NumExpr defaulting to 2 threads.\n","2024-10-21 02:20:52 | INFO | fairseq.tasks.text_to_speech | Please install tensorboardX: pip install tensorboardX\n","2024-10-21 02:20:54 | INFO | fairseq_cli.train | {'_name': None, 'common': {'_name': None, 'no_progress_bar': False, 'log_interval': 100, 'log_format': None, 'log_file': None, 'aim_repo': None, 'aim_run_hash': None, 'tensorboard_logdir': None, 'wandb_project': None, 'azureml_logging': False, 'seed': 1, 'cpu': False, 'tpu': False, 'bf16': False, 'memory_efficient_bf16': False, 'fp16': True, 'memory_efficient_fp16': False, 'fp16_no_flatten_grads': False, 'fp16_init_scale': 128, 'fp16_scale_window': None, 'fp16_scale_tolerance': 0.0, 'on_cpu_convert_precision': False, 'min_loss_scale': 0.0001, 'threshold_loss_scale': None, 'amp': False, 'amp_batch_retries': 2, 'amp_init_scale': 128, 'amp_scale_window': None, 'user_dir': None, 'empty_cache_freq': 0, 'all_gather_list_size': 16384, 'model_parallel_size': 1, 'quantization_config_path': None, 'profile': False, 'reset_logging': False, 'suppress_crashes': False, 'use_plasma_view': False, 'plasma_path': '/tmp/plasma'}, 'common_eval': {'_name': None, 'path': None, 'post_process': None, 'quiet': False, 'model_overrides': '{}', 'results_path': None}, 'distributed_training': {'_name': None, 'distributed_world_size': 1, 'distributed_num_procs': 1, 'distributed_rank': 0, 'distributed_backend': 'nccl', 'distributed_init_method': None, 'distributed_port': -1, 'device_id': 0, 'distributed_no_spawn': False, 'ddp_backend': 'pytorch_ddp', 'ddp_comm_hook': 'none', 'bucket_cap_mb': 25, 'fix_batches_to_gpus': False, 'find_unused_parameters': False, 'gradient_as_bucket_view': False, 'fast_stat_sync': False, 'heartbeat_timeout': -1, 'broadcast_buffers': False, 'slowmo_momentum': None, 'slowmo_base_algorithm': 'localsgd', 'localsgd_frequency': 3, 'nprocs_per_node': 1, 'pipeline_model_parallel': False, 'pipeline_balance': None, 'pipeline_devices': None, 'pipeline_chunks': 0, 'pipeline_encoder_balance': None, 'pipeline_encoder_devices': None, 'pipeline_decoder_balance': None, 'pipeline_decoder_devices': None, 'pipeline_checkpoint': 'never', 'zero_sharding': 'none', 'fp16': True, 'memory_efficient_fp16': False, 'tpu': False, 'no_reshard_after_forward': False, 'fp32_reduce_scatter': False, 'cpu_offload': False, 'use_sharded_state': False, 'not_fsdp_flatten_parameters': False}, 'dataset': {'_name': None, 'num_workers': 1, 'skip_invalid_size_inputs_valid_test': False, 'max_tokens': 4096, 'batch_size': None, 'required_batch_size_multiple': 8, 'required_seq_len_multiple': 1, 'dataset_impl': None, 'data_buffer_size': 10, 'train_subset': 'train', 'valid_subset': 'valid', 'combine_valid_subsets': None, 'ignore_unused_valid_subsets': False, 'validate_interval': 1, 'validate_interval_updates': 0, 'validate_after_updates': 0, 'fixed_validation_seed': None, 'disable_validation': False, 'max_tokens_valid': 4096, 'batch_size_valid': None, 'max_valid_steps': None, 'curriculum': 0, 'gen_subset': 'test', 'num_shards': 1, 'shard_id': 0, 'grouped_shuffling': False, 'update_epoch_batch_itr': False, 'update_ordered_indices_seed': False}, 'optimization': {'_name': None, 'max_epoch': 7, 'max_update': 0, 'stop_time_hours': 0.0, 'clip_norm': 0.1, 'sentence_avg': False, 'update_freq': [1], 'lr': [0.001], 'stop_min_lr': -1.0, 'use_bmuf': False, 'skip_remainder_batch': False}, 'checkpoint': {'_name': None, 'save_dir': 'checkpoints/kftt-checkpoints', 'restore_file': 'checkpoint_last.pt', 'continue_once': None, 'finetune_from_model': None, 'reset_dataloader': False, 'reset_lr_scheduler': False, 'reset_meters': False, 'reset_optimizer': False, 'optimizer_overrides': '{}', 'save_interval': 1, 'save_interval_updates': 0, 'keep_interval_updates': -1, 'keep_interval_updates_pattern': -1, 'keep_last_epochs': -1, 'keep_best_checkpoints': -1, 'no_save': False, 'no_epoch_checkpoints': False, 'no_last_checkpoints': False, 'no_save_optimizer_state': False, 'best_checkpoint_metric': 'loss', 'maximize_best_checkpoint_metric': False, 'patience': -1, 'checkpoint_suffix': '', 'checkpoint_shard_count': 1, 'load_checkpoint_on_all_dp_ranks': False, 'write_checkpoints_asynchronously': False, 'model_parallel_size': 1}, 'bmuf': {'_name': None, 'block_lr': 1.0, 'block_momentum': 0.875, 'global_sync_iter': 50, 'warmup_iterations': 500, 'use_nbm': False, 'average_sync': False, 'distributed_world_size': 1}, 'generation': {'_name': None, 'beam': 5, 'nbest': 1, 'max_len_a': 0.0, 'max_len_b': 200, 'min_len': 1, 'match_source_len': False, 'unnormalized': False, 'no_early_stop': False, 'no_beamable_mm': False, 'lenpen': 1.0, 'unkpen': 0.0, 'replace_unk': None, 'sacrebleu': False, 'score_reference': False, 'prefix_size': 0, 'no_repeat_ngram_size': 0, 'sampling': False, 'sampling_topk': -1, 'sampling_topp': -1.0, 'constraints': None, 'temperature': 1.0, 'diverse_beam_groups': -1, 'diverse_beam_strength': 0.5, 'diversity_rate': -1.0, 'print_alignment': None, 'print_step': False, 'lm_path': None, 'lm_weight': 0.0, 'iter_decode_eos_penalty': 0.0, 'iter_decode_max_iter': 10, 'iter_decode_force_max_iter': False, 'iter_decode_with_beam': 1, 'iter_decode_with_external_reranker': False, 'retain_iter_history': False, 'retain_dropout': False, 'retain_dropout_modules': None, 'decoding_format': None, 'no_seed_provided': False, 'eos_token': None}, 'eval_lm': {'_name': None, 'output_word_probs': False, 'output_word_stats': False, 'context_window': 0, 'softmax_batch': 9223372036854775807}, 'interactive': {'_name': None, 'buffer_size': 0, 'input': '-'}, 'model': Namespace(no_progress_bar=False, log_interval=100, log_format=None, log_file=None, aim_repo=None, aim_run_hash=None, tensorboard_logdir=None, wandb_project=None, azureml_logging=False, seed=1, cpu=False, tpu=False, bf16=False, memory_efficient_bf16=False, fp16=True, memory_efficient_fp16=False, fp16_no_flatten_grads=False, fp16_init_scale=128, fp16_scale_window=None, fp16_scale_tolerance=0.0, on_cpu_convert_precision=False, min_loss_scale=0.0001, threshold_loss_scale=None, amp=False, amp_batch_retries=2, amp_init_scale=128, amp_scale_window=None, user_dir=None, empty_cache_freq=0, all_gather_list_size=16384, model_parallel_size=1, quantization_config_path=None, profile=False, reset_logging=False, suppress_crashes=False, use_plasma_view=False, plasma_path='/tmp/plasma', criterion='cross_entropy', tokenizer=None, bpe=None, optimizer='adam', lr_scheduler='inverse_sqrt', scoring='bleu', task='translation', num_workers=1, skip_invalid_size_inputs_valid_test=False, max_tokens=4096, batch_size=None, required_batch_size_multiple=8, required_seq_len_multiple=1, dataset_impl=None, data_buffer_size=10, train_subset='train', valid_subset='valid', combine_valid_subsets=None, ignore_unused_valid_subsets=False, validate_interval=1, validate_interval_updates=0, validate_after_updates=0, fixed_validation_seed=None, disable_validation=False, max_tokens_valid=4096, batch_size_valid=None, max_valid_steps=None, curriculum=0, gen_subset='test', num_shards=1, shard_id=0, grouped_shuffling=False, update_epoch_batch_itr=False, update_ordered_indices_seed=False, distributed_world_size=1, distributed_num_procs=1, distributed_rank=0, distributed_backend='nccl', distributed_init_method=None, distributed_port=-1, device_id=0, distributed_no_spawn=False, ddp_backend='pytorch_ddp', ddp_comm_hook='none', bucket_cap_mb=25, fix_batches_to_gpus=False, find_unused_parameters=False, gradient_as_bucket_view=False, fast_stat_sync=False, heartbeat_timeout=-1, broadcast_buffers=False, slowmo_momentum=None, slowmo_base_algorithm='localsgd', localsgd_frequency=3, nprocs_per_node=1, pipeline_model_parallel=False, pipeline_balance=None, pipeline_devices=None, pipeline_chunks=0, pipeline_encoder_balance=None, pipeline_encoder_devices=None, pipeline_decoder_balance=None, pipeline_decoder_devices=None, pipeline_checkpoint='never', zero_sharding='none', no_reshard_after_forward=False, fp32_reduce_scatter=False, cpu_offload=False, use_sharded_state=False, not_fsdp_flatten_parameters=False, arch='lstm', max_epoch=7, max_update=0, stop_time_hours=0, clip_norm=0.1, sentence_avg=False, update_freq=[1], lr=[0.001], stop_min_lr=-1.0, use_bmuf=False, skip_remainder_batch=False, save_dir='checkpoints/kftt-checkpoints', restore_file='checkpoint_last.pt', continue_once=None, finetune_from_model=None, reset_dataloader=False, reset_lr_scheduler=False, reset_meters=False, reset_optimizer=False, optimizer_overrides='{}', save_interval=1, save_interval_updates=0, keep_interval_updates=-1, keep_interval_updates_pattern=-1, keep_last_epochs=-1, keep_best_checkpoints=-1, no_save=False, no_epoch_checkpoints=False, no_last_checkpoints=False, no_save_optimizer_state=False, best_checkpoint_metric='loss', maximize_best_checkpoint_metric=False, patience=-1, checkpoint_suffix='', checkpoint_shard_count=1, load_checkpoint_on_all_dp_ranks=False, write_checkpoints_asynchronously=False, store_ema=False, ema_decay=0.9999, ema_start_update=0, ema_seed_model=None, ema_update_freq=1, ema_fp32=False, share_decoder_input_output_embed=True, share_all_embeddings=False, data='data-bin/kftt', source_lang=None, target_lang=None, load_alignments=False, left_pad_source=True, left_pad_target=False, max_source_positions=1024, max_target_positions=1024, upsample_primary=-1, truncate_source=False, num_batch_buckets=0, eval_bleu=False, eval_bleu_args='{}', eval_bleu_detok='space', eval_bleu_detok_args='{}', eval_tokenized_bleu=False, eval_bleu_remove_bpe=None, eval_bleu_print_samples=False, adam_betas=(0.9, 0.999), adam_eps=1e-08, weight_decay=0.0, use_old_adam=False, fp16_adam_stats=False, warmup_updates=4000, warmup_init_lr=-1, pad=1, eos=2, unk=3, dropout=0.3, no_seed_provided=False, encoder_embed_dim=512, encoder_embed_path=None, encoder_freeze_embed=False, encoder_hidden_size=512, encoder_layers=1, encoder_bidirectional=False, encoder_dropout_in=0.3, encoder_dropout_out=0.3, decoder_embed_dim=512, decoder_embed_path=None, decoder_freeze_embed=False, decoder_hidden_size=512, decoder_layers=1, decoder_out_embed_dim=512, decoder_attention='1', decoder_dropout_in=0.3, decoder_dropout_out=0.3, adaptive_softmax_cutoff='10000,50000,200000', _name='lstm'), 'task': {'_name': 'translation', 'data': 'data-bin/kftt', 'source_lang': None, 'target_lang': None, 'load_alignments': False, 'left_pad_source': True, 'left_pad_target': False, 'max_source_positions': 1024, 'max_target_positions': 1024, 'upsample_primary': -1, 'truncate_source': False, 'num_batch_buckets': 0, 'train_subset': 'train', 'dataset_impl': None, 'required_seq_len_multiple': 1, 'eval_bleu': False, 'eval_bleu_args': '{}', 'eval_bleu_detok': 'space', 'eval_bleu_detok_args': '{}', 'eval_tokenized_bleu': False, 'eval_bleu_remove_bpe': None, 'eval_bleu_print_samples': False}, 'criterion': {'_name': 'cross_entropy', 'sentence_avg': False}, 'optimizer': {'_name': 'adam', 'adam_betas': [0.9, 0.999], 'adam_eps': 1e-08, 'weight_decay': 0.0, 'use_old_adam': False, 'fp16_adam_stats': False, 'tpu': False, 'lr': [0.001]}, 'lr_scheduler': {'_name': 'inverse_sqrt', 'warmup_updates': 4000, 'warmup_init_lr': -1.0, 'lr': [0.001]}, 'scoring': {'_name': 'bleu', 'pad': 1, 'eos': 2, 'unk': 3}, 'bpe': None, 'tokenizer': None, 'ema': {'_name': None, 'store_ema': False, 'ema_decay': 0.9999, 'ema_start_update': 0, 'ema_seed_model': None, 'ema_update_freq': 1, 'ema_fp32': False}}\n","2024-10-21 02:20:54 | INFO | fairseq.tasks.translation | [ja] dictionary: 146832 types\n","2024-10-21 02:20:54 | INFO | fairseq.tasks.translation | [en] dictionary: 221864 types\n","2024-10-21 02:20:57 | INFO | fairseq_cli.train | LSTMModel(\n","  (encoder): LSTMEncoder(\n","    (dropout_in_module): FairseqDropout()\n","    (dropout_out_module): FairseqDropout()\n","    (embed_tokens): Embedding(146832, 512, padding_idx=1)\n","    (lstm): LSTM(512, 512)\n","  )\n","  (decoder): LSTMDecoder(\n","    (dropout_in_module): FairseqDropout()\n","    (dropout_out_module): FairseqDropout()\n","    (embed_tokens): Embedding(221864, 512, padding_idx=1)\n","    (layers): ModuleList(\n","      (0): LSTMCell(1024, 512)\n","    )\n","    (attention): AttentionLayer(\n","      (input_proj): Linear(in_features=512, out_features=512, bias=False)\n","      (output_proj): Linear(in_features=1024, out_features=512, bias=False)\n","    )\n","  )\n",")\n","2024-10-21 02:20:57 | INFO | fairseq_cli.train | task: TranslationTask\n","2024-10-21 02:20:57 | INFO | fairseq_cli.train | model: LSTMModel\n","2024-10-21 02:20:57 | INFO | fairseq_cli.train | criterion: CrossEntropyCriterion\n","2024-10-21 02:20:57 | INFO | fairseq_cli.train | num. shared model params: 194,809,856 (num. trained: 194,809,856)\n","2024-10-21 02:20:57 | INFO | fairseq_cli.train | num. expert model params: 0 (num. trained: 0)\n","2024-10-21 02:20:57 | INFO | fairseq.data.data_utils | loaded 1,166 examples from: data-bin/kftt/valid.ja-en.ja\n","2024-10-21 02:20:57 | INFO | fairseq.data.data_utils | loaded 1,166 examples from: data-bin/kftt/valid.ja-en.en\n","2024-10-21 02:20:57 | INFO | fairseq.tasks.translation | data-bin/kftt valid ja-en 1166 examples\n","2024-10-21 02:20:58 | INFO | fairseq.trainer | detected shared parameter: decoder.attention.input_proj.bias <- decoder.attention.output_proj.bias\n","2024-10-21 02:20:58 | INFO | fairseq.utils | ***********************CUDA enviroments for all 1 workers***********************\n","2024-10-21 02:20:58 | INFO | fairseq.utils | rank   0: capabilities =  7.5  ; total memory = 14.748 GB ; name = Tesla T4                                \n","2024-10-21 02:20:58 | INFO | fairseq.utils | ***********************CUDA enviroments for all 1 workers***********************\n","2024-10-21 02:20:58 | INFO | fairseq_cli.train | training on 1 devices (GPUs/TPUs)\n","2024-10-21 02:20:58 | INFO | fairseq_cli.train | max tokens per device = 4096 and max sentences per device = None\n","2024-10-21 02:20:58 | INFO | fairseq.trainer | Preparing to load checkpoint checkpoints/kftt-checkpoints/checkpoint_last.pt\n","2024-10-21 02:20:58 | INFO | fairseq.trainer | No existing checkpoint found checkpoints/kftt-checkpoints/checkpoint_last.pt\n","2024-10-21 02:20:58 | INFO | fairseq.trainer | loading train data for epoch 1\n","2024-10-21 02:20:58 | INFO | fairseq.data.data_utils | loaded 440,288 examples from: data-bin/kftt/train.ja-en.ja\n","2024-10-21 02:20:59 | INFO | fairseq.data.data_utils | loaded 440,288 examples from: data-bin/kftt/train.ja-en.en\n","2024-10-21 02:20:59 | INFO | fairseq.tasks.translation | data-bin/kftt train ja-en 440288 examples\n","2024-10-21 02:20:59 | INFO | fairseq.data.iterators | grouped total_num_itrs = 3599\n","epoch 001:   0% 0/3599 [00:00<?, ?it/s]2024-10-21 02:20:59 | INFO | fairseq.trainer | begin training epoch 1\n","2024-10-21 02:20:59 | INFO | fairseq_cli.train | Start iterating over samples\n","/usr/local/lib/python3.10/dist-packages/fairseq/tasks/fairseq_task.py:514: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n","  with torch.cuda.amp.autocast(enabled=(isinstance(optimizer, AMPOptimizer))):\n","epoch 001:   4% 152/3599 [00:53<19:19,  2.97it/s, loss=17.504, ppl=185917, wps=10028.3, ups=2.93, wpb=3421.6, bsz=115.8, num_updates=100, lr=2.5e-05, gnorm=1.123, clip=100, loss_scale=128, train_wall=35, gb_free=3.6, wall=37]2024-10-21 02:21:53 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 64.0\n","epoch 001:  16% 559/3599 [03:18<17:53,  2.83it/s, loss=10.427, ppl=1376.38, wps=8837.5, ups=2.75, wpb=3217.4, bsz=115.8, num_updates=500, lr=0.000125, gnorm=1.431, clip=100, loss_scale=64, train_wall=36, gb_free=4.2, wall=179]2024-10-21 02:24:18 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0\n","epoch 001: 100% 3598/3599 [21:23<00:00,  2.84it/s, loss=6.336, ppl=80.77, wps=8797.6, ups=2.72, wpb=3231.8, bsz=95.6, num_updates=3500, lr=0.000875, gnorm=0.548, clip=100, loss_scale=32, train_wall=36, gb_free=3.2, wall=1250]2024-10-21 02:42:23 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n","\n","epoch 001 | valid on 'valid' subset:   0% 0/13 [00:00<?, ?it/s]\u001b[A\n","epoch 001 | valid on 'valid' subset:   8% 1/13 [00:00<00:02,  5.55it/s]\u001b[A\n","epoch 001 | valid on 'valid' subset:  15% 2/13 [00:00<00:01,  7.04it/s]\u001b[A\n","epoch 001 | valid on 'valid' subset:  31% 4/13 [00:00<00:01,  8.71it/s]\u001b[A\n","epoch 001 | valid on 'valid' subset:  38% 5/13 [00:00<00:00,  8.80it/s]\u001b[A\n","epoch 001 | valid on 'valid' subset:  46% 6/13 [00:00<00:00,  8.84it/s]\u001b[A\n","epoch 001 | valid on 'valid' subset:  54% 7/13 [00:00<00:00,  8.94it/s]\u001b[A\n","epoch 001 | valid on 'valid' subset:  62% 8/13 [00:00<00:00,  8.95it/s]\u001b[A\n","epoch 001 | valid on 'valid' subset:  69% 9/13 [00:01<00:00,  8.98it/s]\u001b[A\n","epoch 001 | valid on 'valid' subset:  77% 10/13 [00:01<00:00,  8.66it/s]\u001b[A\n","epoch 001 | valid on 'valid' subset:  85% 11/13 [00:01<00:00,  8.46it/s]\u001b[A\n","epoch 001 | valid on 'valid' subset:  92% 12/13 [00:01<00:00,  8.09it/s]\u001b[A\n","                                                                        \u001b[A2024-10-21 02:42:25 | INFO | valid | epoch 001 | valid on 'valid' subset | loss 6.293 | ppl 78.4 | wps 17108.9 | wpb 1959.6 | bsz 89.7 | num_updates 3597\n","2024-10-21 02:42:25 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 1 @ 3597 updates\n","2024-10-21 02:42:25 | INFO | fairseq.trainer | Saving checkpoint to /content/checkpoints/kftt-checkpoints/checkpoint1.pt\n","2024-10-21 02:42:40 | INFO | fairseq.trainer | Finished saving checkpoint to /content/checkpoints/kftt-checkpoints/checkpoint1.pt\n","2024-10-21 02:43:10 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/kftt-checkpoints/checkpoint1.pt (epoch 1 @ 3597 updates, score 6.293) (writing took 45.32350104400007 seconds)\n","2024-10-21 02:43:10 | INFO | fairseq_cli.train | end of epoch 1 (average epoch stats below)\n","2024-10-21 02:43:10 | INFO | train | epoch 001 | loss 8.554 | ppl 375.8 | wps 9009 | ups 2.71 | wpb 3329 | bsz 121.8 | num_updates 3597 | lr 0.00089925 | gnorm 0.823 | clip 100 | loss_scale 32 | train_wall 1271 | gb_free 4.1 | wall 1332\n","2024-10-21 02:43:10 | INFO | fairseq.data.iterators | grouped total_num_itrs = 3599\n","epoch 002:   0% 0/3599 [00:00<?, ?it/s]2024-10-21 02:43:10 | INFO | fairseq.trainer | begin training epoch 2\n","2024-10-21 02:43:10 | INFO | fairseq_cli.train | Start iterating over samples\n","epoch 002: 100% 3598/3599 [21:21<00:00,  3.06it/s, loss=4.929, ppl=30.47, wps=9203.6, ups=2.79, wpb=3299.6, bsz=122.6, num_updates=7100, lr=0.000750587, gnorm=0.497, clip=100, loss_scale=32, train_wall=36, gb_free=3.1, wall=2579]2024-10-21 03:04:32 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n","\n","epoch 002 | valid on 'valid' subset:   0% 0/13 [00:00<?, ?it/s]\u001b[A\n","epoch 002 | valid on 'valid' subset:   8% 1/13 [00:00<00:01,  8.10it/s]\u001b[A\n","epoch 002 | valid on 'valid' subset:  15% 2/13 [00:00<00:01,  8.46it/s]\u001b[A\n","epoch 002 | valid on 'valid' subset:  31% 4/13 [00:00<00:00,  9.71it/s]\u001b[A\n","epoch 002 | valid on 'valid' subset:  38% 5/13 [00:00<00:00,  9.77it/s]\u001b[A\n","epoch 002 | valid on 'valid' subset:  54% 7/13 [00:00<00:00,  9.81it/s]\u001b[A\n","epoch 002 | valid on 'valid' subset:  62% 8/13 [00:00<00:00,  9.70it/s]\u001b[A\n","epoch 002 | valid on 'valid' subset:  69% 9/13 [00:00<00:00,  9.24it/s]\u001b[A\n","epoch 002 | valid on 'valid' subset:  77% 10/13 [00:01<00:00,  8.97it/s]\u001b[A\n","epoch 002 | valid on 'valid' subset:  85% 11/13 [00:01<00:00,  8.89it/s]\u001b[A\n","epoch 002 | valid on 'valid' subset:  92% 12/13 [00:01<00:00,  8.69it/s]\u001b[A\n","                                                                        \u001b[A2024-10-21 03:04:33 | INFO | valid | epoch 002 | valid on 'valid' subset | loss 5.035 | ppl 32.79 | wps 18125.3 | wpb 1959.6 | bsz 89.7 | num_updates 7196 | best_loss 5.035\n","2024-10-21 03:04:33 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 2 @ 7196 updates\n","2024-10-21 03:04:33 | INFO | fairseq.trainer | Saving checkpoint to /content/checkpoints/kftt-checkpoints/checkpoint2.pt\n","2024-10-21 03:04:52 | INFO | fairseq.trainer | Finished saving checkpoint to /content/checkpoints/kftt-checkpoints/checkpoint2.pt\n","2024-10-21 03:05:29 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/kftt-checkpoints/checkpoint2.pt (epoch 2 @ 7196 updates, score 5.035) (writing took 55.49269354600028 seconds)\n","2024-10-21 03:05:29 | INFO | fairseq_cli.train | end of epoch 2 (average epoch stats below)\n","2024-10-21 03:05:29 | INFO | train | epoch 002 | loss 5.363 | ppl 41.14 | wps 8950.2 | ups 2.69 | wpb 3329.2 | bsz 122.3 | num_updates 7196 | lr 0.000745563 | gnorm 0.507 | clip 100 | loss_scale 32 | train_wall 1268 | gb_free 3 | wall 2670\n","2024-10-21 03:05:29 | INFO | fairseq.data.iterators | grouped total_num_itrs = 3599\n","epoch 003:   0% 0/3599 [00:00<?, ?it/s]2024-10-21 03:05:29 | INFO | fairseq.trainer | begin training epoch 3\n","2024-10-21 03:05:29 | INFO | fairseq_cli.train | Start iterating over samples\n","epoch 003: 100% 3598/3599 [21:14<00:00,  2.93it/s, loss=4.385, ppl=20.9, wps=9534.8, ups=2.82, wpb=3377.4, bsz=119.2, num_updates=10700, lr=0.000611418, gnorm=0.482, clip=100, loss_scale=32, train_wall=35, gb_free=3.2, wall=3912]2024-10-21 03:26:44 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n","\n","epoch 003 | valid on 'valid' subset:   0% 0/13 [00:00<?, ?it/s]\u001b[A\n","epoch 003 | valid on 'valid' subset:   8% 1/13 [00:00<00:01,  8.00it/s]\u001b[A\n","epoch 003 | valid on 'valid' subset:  15% 2/13 [00:00<00:01,  8.37it/s]\u001b[A\n","epoch 003 | valid on 'valid' subset:  31% 4/13 [00:00<00:00,  9.44it/s]\u001b[A\n","epoch 003 | valid on 'valid' subset:  38% 5/13 [00:00<00:00,  9.58it/s]\u001b[A\n","epoch 003 | valid on 'valid' subset:  46% 6/13 [00:00<00:00,  9.29it/s]\u001b[A\n","epoch 003 | valid on 'valid' subset:  54% 7/13 [00:00<00:00,  9.28it/s]\u001b[A\n","epoch 003 | valid on 'valid' subset:  62% 8/13 [00:00<00:00,  9.08it/s]\u001b[A\n","epoch 003 | valid on 'valid' subset:  69% 9/13 [00:00<00:00,  9.00it/s]\u001b[A\n","epoch 003 | valid on 'valid' subset:  77% 10/13 [00:01<00:00,  8.77it/s]\u001b[A\n","epoch 003 | valid on 'valid' subset:  85% 11/13 [00:01<00:00,  8.42it/s]\u001b[A\n","epoch 003 | valid on 'valid' subset:  92% 12/13 [00:01<00:00,  7.97it/s]\u001b[A\n","epoch 003 | valid on 'valid' subset: 100% 13/13 [00:01<00:00,  8.38it/s]\u001b[A\n","                                                                        \u001b[A2024-10-21 03:26:45 | INFO | valid | epoch 003 | valid on 'valid' subset | loss 4.708 | ppl 26.14 | wps 16940.9 | wpb 1959.6 | bsz 89.7 | num_updates 10795 | best_loss 4.708\n","2024-10-21 03:26:45 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 3 @ 10795 updates\n","2024-10-21 03:26:45 | INFO | fairseq.trainer | Saving checkpoint to /content/checkpoints/kftt-checkpoints/checkpoint3.pt\n","2024-10-21 03:27:03 | INFO | fairseq.trainer | Finished saving checkpoint to /content/checkpoints/kftt-checkpoints/checkpoint3.pt\n","2024-10-21 03:27:40 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/kftt-checkpoints/checkpoint3.pt (epoch 3 @ 10795 updates, score 4.708) (writing took 54.95836374800001 seconds)\n","2024-10-21 03:27:40 | INFO | fairseq_cli.train | end of epoch 3 (average epoch stats below)\n","2024-10-21 03:27:40 | INFO | train | epoch 003 | loss 4.473 | ppl 22.21 | wps 8998.1 | ups 2.7 | wpb 3329.2 | bsz 122.3 | num_updates 10795 | lr 0.000608722 | gnorm 0.499 | clip 100 | loss_scale 32 | train_wall 1262 | gb_free 2.9 | wall 4002\n","2024-10-21 03:27:40 | INFO | fairseq.data.iterators | grouped total_num_itrs = 3599\n","epoch 004:   0% 0/3599 [00:00<?, ?it/s]2024-10-21 03:27:40 | INFO | fairseq.trainer | begin training epoch 4\n","2024-10-21 03:27:40 | INFO | fairseq_cli.train | Start iterating over samples\n","epoch 004: 100% 3598/3599 [21:11<00:00,  2.69it/s, loss=3.997, ppl=15.96, wps=9424.2, ups=2.88, wpb=3268.2, bsz=126.3, num_updates=14300, lr=0.000528886, gnorm=0.508, clip=100, loss_scale=32, train_wall=34, gb_free=3, wall=5240]2024-10-21 03:48:52 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n","\n","epoch 004 | valid on 'valid' subset:   0% 0/13 [00:00<?, ?it/s]\u001b[A\n","epoch 004 | valid on 'valid' subset:   8% 1/13 [00:00<00:01,  8.11it/s]\u001b[A\n","epoch 004 | valid on 'valid' subset:  15% 2/13 [00:00<00:01,  8.39it/s]\u001b[A\n","epoch 004 | valid on 'valid' subset:  31% 4/13 [00:00<00:00,  9.78it/s]\u001b[A\n","epoch 004 | valid on 'valid' subset:  38% 5/13 [00:00<00:00,  9.81it/s]\u001b[A\n","epoch 004 | valid on 'valid' subset:  46% 6/13 [00:00<00:00,  9.70it/s]\u001b[A\n","epoch 004 | valid on 'valid' subset:  54% 7/13 [00:00<00:00,  9.70it/s]\u001b[A\n","epoch 004 | valid on 'valid' subset:  62% 8/13 [00:00<00:00,  9.60it/s]\u001b[A\n","epoch 004 | valid on 'valid' subset:  69% 9/13 [00:00<00:00,  9.00it/s]\u001b[A\n","epoch 004 | valid on 'valid' subset:  77% 10/13 [00:01<00:00,  8.79it/s]\u001b[A\n","epoch 004 | valid on 'valid' subset:  85% 11/13 [00:01<00:00,  8.78it/s]\u001b[A\n","epoch 004 | valid on 'valid' subset:  92% 12/13 [00:01<00:00,  8.57it/s]\u001b[A\n","                                                                        \u001b[A2024-10-21 03:48:54 | INFO | valid | epoch 004 | valid on 'valid' subset | loss 4.555 | ppl 23.51 | wps 17988.8 | wpb 1959.6 | bsz 89.7 | num_updates 14394 | best_loss 4.555\n","2024-10-21 03:48:54 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 4 @ 14394 updates\n","2024-10-21 03:48:54 | INFO | fairseq.trainer | Saving checkpoint to /content/checkpoints/kftt-checkpoints/checkpoint4.pt\n","2024-10-21 03:49:11 | INFO | fairseq.trainer | Finished saving checkpoint to /content/checkpoints/kftt-checkpoints/checkpoint4.pt\n","2024-10-21 03:49:49 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/kftt-checkpoints/checkpoint4.pt (epoch 4 @ 14394 updates, score 4.555) (writing took 55.36690007800007 seconds)\n","2024-10-21 03:49:49 | INFO | fairseq_cli.train | end of epoch 4 (average epoch stats below)\n","2024-10-21 03:49:49 | INFO | train | epoch 004 | loss 4.048 | ppl 16.54 | wps 9016.6 | ups 2.71 | wpb 3329.2 | bsz 122.3 | num_updates 14394 | lr 0.000527156 | gnorm 0.507 | clip 100 | loss_scale 32 | train_wall 1259 | gb_free 4.2 | wall 5331\n","2024-10-21 03:49:49 | INFO | fairseq.data.iterators | grouped total_num_itrs = 3599\n","epoch 005:   0% 0/3599 [00:00<?, ?it/s]2024-10-21 03:49:49 | INFO | fairseq.trainer | begin training epoch 5\n","2024-10-21 03:49:49 | INFO | fairseq_cli.train | Start iterating over samples\n","epoch 005: 100% 3598/3599 [21:10<00:00,  3.34it/s, loss=3.839, ppl=14.31, wps=9537.2, ups=2.77, wpb=3437.8, bsz=102.6, num_updates=17900, lr=0.000472719, gnorm=0.496, clip=100, loss_scale=64, train_wall=36, gb_free=5.6, wall=6569]2024-10-21 04:11:00 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n","\n","epoch 005 | valid on 'valid' subset:   0% 0/13 [00:00<?, ?it/s]\u001b[A\n","epoch 005 | valid on 'valid' subset:   8% 1/13 [00:00<00:01,  7.90it/s]\u001b[A\n","epoch 005 | valid on 'valid' subset:  15% 2/13 [00:00<00:01,  8.46it/s]\u001b[A\n","epoch 005 | valid on 'valid' subset:  31% 4/13 [00:00<00:00,  9.84it/s]\u001b[A\n","epoch 005 | valid on 'valid' subset:  38% 5/13 [00:00<00:00,  9.76it/s]\u001b[A\n","epoch 005 | valid on 'valid' subset:  46% 6/13 [00:00<00:00,  9.66it/s]\u001b[A\n","epoch 005 | valid on 'valid' subset:  54% 7/13 [00:00<00:00,  9.74it/s]\u001b[A\n","epoch 005 | valid on 'valid' subset:  62% 8/13 [00:00<00:00,  9.64it/s]\u001b[A\n","epoch 005 | valid on 'valid' subset:  69% 9/13 [00:00<00:00,  8.98it/s]\u001b[A\n","epoch 005 | valid on 'valid' subset:  77% 10/13 [00:01<00:00,  8.70it/s]\u001b[A\n","epoch 005 | valid on 'valid' subset:  85% 11/13 [00:01<00:00,  8.36it/s]\u001b[A\n","epoch 005 | valid on 'valid' subset:  92% 12/13 [00:01<00:00,  8.28it/s]\u001b[A\n","                                                                        \u001b[A2024-10-21 04:11:02 | INFO | valid | epoch 005 | valid on 'valid' subset | loss 4.493 | ppl 22.51 | wps 17735.9 | wpb 1959.6 | bsz 89.7 | num_updates 17993 | best_loss 4.493\n","2024-10-21 04:11:02 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 5 @ 17993 updates\n","2024-10-21 04:11:02 | INFO | fairseq.trainer | Saving checkpoint to /content/checkpoints/kftt-checkpoints/checkpoint5.pt\n","2024-10-21 04:11:26 | INFO | fairseq.trainer | Finished saving checkpoint to /content/checkpoints/kftt-checkpoints/checkpoint5.pt\n","2024-10-21 04:12:07 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/kftt-checkpoints/checkpoint5.pt (epoch 5 @ 17993 updates, score 4.493) (writing took 65.56102124100016 seconds)\n","2024-10-21 04:12:07 | INFO | fairseq_cli.train | end of epoch 5 (average epoch stats below)\n","2024-10-21 04:12:07 | INFO | train | epoch 005 | loss 3.778 | ppl 13.72 | wps 8954.5 | ups 2.69 | wpb 3329.2 | bsz 122.3 | num_updates 17993 | lr 0.000471496 | gnorm 0.518 | clip 100 | loss_scale 64 | train_wall 1257 | gb_free 3.8 | wall 6669\n","2024-10-21 04:12:07 | INFO | fairseq.data.iterators | grouped total_num_itrs = 3599\n","epoch 006:   0% 0/3599 [00:00<?, ?it/s]2024-10-21 04:12:07 | INFO | fairseq.trainer | begin training epoch 6\n","2024-10-21 04:12:07 | INFO | fairseq_cli.train | Start iterating over samples\n","epoch 006: 100% 3598/3599 [21:12<00:00,  3.04it/s, loss=3.497, ppl=11.29, wps=10058.8, ups=2.86, wpb=3514.1, bsz=153.1, num_updates=21500, lr=0.000431331, gnorm=0.498, clip=100, loss_scale=64, train_wall=35, gb_free=4.6, wall=7909]2024-10-21 04:33:20 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n","\n","epoch 006 | valid on 'valid' subset:   0% 0/13 [00:00<?, ?it/s]\u001b[A\n","epoch 006 | valid on 'valid' subset:   8% 1/13 [00:00<00:01,  8.13it/s]\u001b[A\n","epoch 006 | valid on 'valid' subset:  15% 2/13 [00:00<00:01,  8.05it/s]\u001b[A\n","epoch 006 | valid on 'valid' subset:  31% 4/13 [00:00<00:00,  9.56it/s]\u001b[A\n","epoch 006 | valid on 'valid' subset:  38% 5/13 [00:00<00:00,  9.65it/s]\u001b[A\n","epoch 006 | valid on 'valid' subset:  46% 6/13 [00:00<00:00,  9.69it/s]\u001b[A\n","epoch 006 | valid on 'valid' subset:  54% 7/13 [00:00<00:00,  9.77it/s]\u001b[A\n","epoch 006 | valid on 'valid' subset:  62% 8/13 [00:00<00:00,  9.64it/s]\u001b[A\n","epoch 006 | valid on 'valid' subset:  69% 9/13 [00:00<00:00,  9.48it/s]\u001b[A\n","epoch 006 | valid on 'valid' subset:  77% 10/13 [00:01<00:00,  9.13it/s]\u001b[A\n","epoch 006 | valid on 'valid' subset:  85% 11/13 [00:01<00:00,  8.91it/s]\u001b[A\n","epoch 006 | valid on 'valid' subset:  92% 12/13 [00:01<00:00,  8.61it/s]\u001b[A\n","                                                                        \u001b[A2024-10-21 04:33:21 | INFO | valid | epoch 006 | valid on 'valid' subset | loss 4.449 | ppl 21.84 | wps 18126.1 | wpb 1959.6 | bsz 89.7 | num_updates 21592 | best_loss 4.449\n","2024-10-21 04:33:21 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 6 @ 21592 updates\n","2024-10-21 04:33:21 | INFO | fairseq.trainer | Saving checkpoint to /content/checkpoints/kftt-checkpoints/checkpoint6.pt\n","2024-10-21 04:33:39 | INFO | fairseq.trainer | Finished saving checkpoint to /content/checkpoints/kftt-checkpoints/checkpoint6.pt\n","2024-10-21 04:34:16 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/kftt-checkpoints/checkpoint6.pt (epoch 6 @ 21592 updates, score 4.449) (writing took 54.68068696999944 seconds)\n","2024-10-21 04:34:16 | INFO | fairseq_cli.train | end of epoch 6 (average epoch stats below)\n","2024-10-21 04:34:16 | INFO | train | epoch 006 | loss 3.581 | ppl 11.97 | wps 9015.8 | ups 2.71 | wpb 3329.2 | bsz 122.3 | num_updates 21592 | lr 0.000430411 | gnorm 0.525 | clip 100 | loss_scale 64 | train_wall 1260 | gb_free 3 | wall 7998\n","2024-10-21 04:34:18 | INFO | fairseq.data.iterators | grouped total_num_itrs = 3599\n","epoch 007:   0% 0/3599 [00:00<?, ?it/s]2024-10-21 04:34:18 | INFO | fairseq.trainer | begin training epoch 7\n","2024-10-21 04:34:18 | INFO | fairseq_cli.train | Start iterating over samples\n","epoch 007: 100% 3598/3599 [21:11<00:00,  2.91it/s, loss=3.479, ppl=11.15, wps=9755.8, ups=2.87, wpb=3401, bsz=125, num_updates=25100, lr=0.000399202, gnorm=0.529, clip=100, loss_scale=64, train_wall=34, gb_free=3, wall=9240]2024-10-21 04:55:30 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n","\n","epoch 007 | valid on 'valid' subset:   0% 0/13 [00:00<?, ?it/s]\u001b[A\n","epoch 007 | valid on 'valid' subset:   8% 1/13 [00:00<00:01,  7.94it/s]\u001b[A\n","epoch 007 | valid on 'valid' subset:  15% 2/13 [00:00<00:01,  8.53it/s]\u001b[A\n","epoch 007 | valid on 'valid' subset:  31% 4/13 [00:00<00:00,  9.74it/s]\u001b[A\n","epoch 007 | valid on 'valid' subset:  38% 5/13 [00:00<00:00,  9.81it/s]\u001b[A\n","epoch 007 | valid on 'valid' subset:  46% 6/13 [00:00<00:00,  9.80it/s]\u001b[A\n","epoch 007 | valid on 'valid' subset:  54% 7/13 [00:00<00:00,  9.78it/s]\u001b[A\n","epoch 007 | valid on 'valid' subset:  62% 8/13 [00:00<00:00,  9.60it/s]\u001b[A\n","epoch 007 | valid on 'valid' subset:  69% 9/13 [00:00<00:00,  9.02it/s]\u001b[A\n","epoch 007 | valid on 'valid' subset:  77% 10/13 [00:01<00:00,  8.81it/s]\u001b[A\n","epoch 007 | valid on 'valid' subset:  85% 11/13 [00:01<00:00,  8.66it/s]\u001b[A\n","epoch 007 | valid on 'valid' subset:  92% 12/13 [00:01<00:00,  8.52it/s]\u001b[A\n","                                                                        \u001b[A2024-10-21 04:55:31 | INFO | valid | epoch 007 | valid on 'valid' subset | loss 4.421 | ppl 21.43 | wps 18001 | wpb 1959.6 | bsz 89.7 | num_updates 25191 | best_loss 4.421\n","2024-10-21 04:55:31 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 7 @ 25191 updates\n","2024-10-21 04:55:31 | INFO | fairseq.trainer | Saving checkpoint to /content/checkpoints/kftt-checkpoints/checkpoint7.pt\n","2024-10-21 04:55:53 | INFO | fairseq.trainer | Finished saving checkpoint to /content/checkpoints/kftt-checkpoints/checkpoint7.pt\n","2024-10-21 04:56:59 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/kftt-checkpoints/checkpoint7.pt (epoch 7 @ 25191 updates, score 4.421) (writing took 87.7212534479986 seconds)\n","2024-10-21 04:56:59 | INFO | fairseq_cli.train | end of epoch 7 (average epoch stats below)\n","2024-10-21 04:56:59 | INFO | train | epoch 007 | loss 3.429 | ppl 10.77 | wps 8791.4 | ups 2.64 | wpb 3329.2 | bsz 122.3 | num_updates 25191 | lr 0.000398481 | gnorm 0.534 | clip 100 | loss_scale 64 | train_wall 1259 | gb_free 3.2 | wall 9361\n","2024-10-21 04:56:59 | INFO | fairseq_cli.train | done training in 9360.0 seconds\n"]}]},{"cell_type":"code","source":["# 92\n","# 機械翻訳モデルを用いて日本語を英語に翻訳\n","!echo 'こんにちは、今日はいい天気ですね。' > input.ja\n","!fairseq-interactive data-bin/kftt \\\n","    --path checkpoints/kftt-checkpoints/checkpoint_best.pt \\\n","    --beam 5 --source-lang ja --target-lang en \\\n","    --input input.ja"],"metadata":{"id":"DjfkJW8m5q29","executionInfo":{"status":"ok","timestamp":1729486667953,"user_tz":-540,"elapsed":34168,"user":{"displayName":"shuto u","userId":"05197020628006578565"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"bb3c410d-873c-4f04-ddfc-261ab5bf0b00"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["2024-10-21 04:57:17.696532: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n","2024-10-21 04:57:17.723706: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n","2024-10-21 04:57:17.732782: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","2024-10-21 04:57:17.761181: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n","To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n","2024-10-21 04:57:19.201762: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n","2024-10-21 04:57:23 | INFO | fairseq.tasks.text_to_speech | Please install tensorboardX: pip install tensorboardX\n","2024-10-21 04:57:25 | INFO | fairseq_cli.interactive | {'_name': None, 'common': {'_name': None, 'no_progress_bar': False, 'log_interval': 100, 'log_format': None, 'log_file': None, 'aim_repo': None, 'aim_run_hash': None, 'tensorboard_logdir': None, 'wandb_project': None, 'azureml_logging': False, 'seed': 1, 'cpu': False, 'tpu': False, 'bf16': False, 'memory_efficient_bf16': False, 'fp16': False, 'memory_efficient_fp16': False, 'fp16_no_flatten_grads': False, 'fp16_init_scale': 128, 'fp16_scale_window': None, 'fp16_scale_tolerance': 0.0, 'on_cpu_convert_precision': False, 'min_loss_scale': 0.0001, 'threshold_loss_scale': None, 'amp': False, 'amp_batch_retries': 2, 'amp_init_scale': 128, 'amp_scale_window': None, 'user_dir': None, 'empty_cache_freq': 0, 'all_gather_list_size': 16384, 'model_parallel_size': 1, 'quantization_config_path': None, 'profile': False, 'reset_logging': False, 'suppress_crashes': False, 'use_plasma_view': False, 'plasma_path': '/tmp/plasma'}, 'common_eval': {'_name': None, 'path': 'checkpoints/kftt-checkpoints/checkpoint_best.pt', 'post_process': None, 'quiet': False, 'model_overrides': '{}', 'results_path': None}, 'distributed_training': {'_name': None, 'distributed_world_size': 1, 'distributed_num_procs': 1, 'distributed_rank': 0, 'distributed_backend': 'nccl', 'distributed_init_method': None, 'distributed_port': -1, 'device_id': 0, 'distributed_no_spawn': False, 'ddp_backend': 'pytorch_ddp', 'ddp_comm_hook': 'none', 'bucket_cap_mb': 25, 'fix_batches_to_gpus': False, 'find_unused_parameters': False, 'gradient_as_bucket_view': False, 'fast_stat_sync': False, 'heartbeat_timeout': -1, 'broadcast_buffers': False, 'slowmo_momentum': None, 'slowmo_base_algorithm': 'localsgd', 'localsgd_frequency': 3, 'nprocs_per_node': 1, 'pipeline_model_parallel': False, 'pipeline_balance': None, 'pipeline_devices': None, 'pipeline_chunks': 0, 'pipeline_encoder_balance': None, 'pipeline_encoder_devices': None, 'pipeline_decoder_balance': None, 'pipeline_decoder_devices': None, 'pipeline_checkpoint': 'never', 'zero_sharding': 'none', 'fp16': False, 'memory_efficient_fp16': False, 'tpu': False, 'no_reshard_after_forward': False, 'fp32_reduce_scatter': False, 'cpu_offload': False, 'use_sharded_state': False, 'not_fsdp_flatten_parameters': False}, 'dataset': {'_name': None, 'num_workers': 1, 'skip_invalid_size_inputs_valid_test': False, 'max_tokens': None, 'batch_size': 1, 'required_batch_size_multiple': 8, 'required_seq_len_multiple': 1, 'dataset_impl': None, 'data_buffer_size': 10, 'train_subset': 'train', 'valid_subset': 'valid', 'combine_valid_subsets': None, 'ignore_unused_valid_subsets': False, 'validate_interval': 1, 'validate_interval_updates': 0, 'validate_after_updates': 0, 'fixed_validation_seed': None, 'disable_validation': False, 'max_tokens_valid': None, 'batch_size_valid': None, 'max_valid_steps': None, 'curriculum': 0, 'gen_subset': 'test', 'num_shards': 1, 'shard_id': 0, 'grouped_shuffling': False, 'update_epoch_batch_itr': False, 'update_ordered_indices_seed': False}, 'optimization': {'_name': None, 'max_epoch': 0, 'max_update': 0, 'stop_time_hours': 0.0, 'clip_norm': 0.0, 'sentence_avg': False, 'update_freq': [1], 'lr': [0.25], 'stop_min_lr': -1.0, 'use_bmuf': False, 'skip_remainder_batch': False}, 'checkpoint': {'_name': None, 'save_dir': 'checkpoints', 'restore_file': 'checkpoint_last.pt', 'continue_once': None, 'finetune_from_model': None, 'reset_dataloader': False, 'reset_lr_scheduler': False, 'reset_meters': False, 'reset_optimizer': False, 'optimizer_overrides': '{}', 'save_interval': 1, 'save_interval_updates': 0, 'keep_interval_updates': -1, 'keep_interval_updates_pattern': -1, 'keep_last_epochs': -1, 'keep_best_checkpoints': -1, 'no_save': False, 'no_epoch_checkpoints': False, 'no_last_checkpoints': False, 'no_save_optimizer_state': False, 'best_checkpoint_metric': 'loss', 'maximize_best_checkpoint_metric': False, 'patience': -1, 'checkpoint_suffix': '', 'checkpoint_shard_count': 1, 'load_checkpoint_on_all_dp_ranks': False, 'write_checkpoints_asynchronously': False, 'model_parallel_size': 1}, 'bmuf': {'_name': None, 'block_lr': 1.0, 'block_momentum': 0.875, 'global_sync_iter': 50, 'warmup_iterations': 500, 'use_nbm': False, 'average_sync': False, 'distributed_world_size': 1}, 'generation': {'_name': None, 'beam': 5, 'nbest': 1, 'max_len_a': 0.0, 'max_len_b': 200, 'min_len': 1, 'match_source_len': False, 'unnormalized': False, 'no_early_stop': False, 'no_beamable_mm': False, 'lenpen': 1.0, 'unkpen': 0.0, 'replace_unk': None, 'sacrebleu': False, 'score_reference': False, 'prefix_size': 0, 'no_repeat_ngram_size': 0, 'sampling': False, 'sampling_topk': -1, 'sampling_topp': -1.0, 'constraints': None, 'temperature': 1.0, 'diverse_beam_groups': -1, 'diverse_beam_strength': 0.5, 'diversity_rate': -1.0, 'print_alignment': None, 'print_step': False, 'lm_path': None, 'lm_weight': 0.0, 'iter_decode_eos_penalty': 0.0, 'iter_decode_max_iter': 10, 'iter_decode_force_max_iter': False, 'iter_decode_with_beam': 1, 'iter_decode_with_external_reranker': False, 'retain_iter_history': False, 'retain_dropout': False, 'retain_dropout_modules': None, 'decoding_format': None, 'no_seed_provided': False, 'eos_token': None}, 'eval_lm': {'_name': None, 'output_word_probs': False, 'output_word_stats': False, 'context_window': 0, 'softmax_batch': 9223372036854775807}, 'interactive': {'_name': None, 'buffer_size': 1, 'input': 'input.ja'}, 'model': None, 'task': {'_name': 'translation', 'data': 'data-bin/kftt', 'source_lang': 'ja', 'target_lang': 'en', 'load_alignments': False, 'left_pad_source': True, 'left_pad_target': False, 'max_source_positions': 1024, 'max_target_positions': 1024, 'upsample_primary': -1, 'truncate_source': False, 'num_batch_buckets': 0, 'train_subset': 'train', 'dataset_impl': None, 'required_seq_len_multiple': 1, 'eval_bleu': False, 'eval_bleu_args': '{}', 'eval_bleu_detok': 'space', 'eval_bleu_detok_args': '{}', 'eval_tokenized_bleu': False, 'eval_bleu_remove_bpe': None, 'eval_bleu_print_samples': False}, 'criterion': {'_name': 'cross_entropy', 'sentence_avg': True}, 'optimizer': None, 'lr_scheduler': {'_name': 'fixed', 'force_anneal': None, 'lr_shrink': 0.1, 'warmup_updates': 0, 'lr': [0.25]}, 'scoring': {'_name': 'bleu', 'pad': 1, 'eos': 2, 'unk': 3}, 'bpe': None, 'tokenizer': None, 'ema': {'_name': None, 'store_ema': False, 'ema_decay': 0.9999, 'ema_start_update': 0, 'ema_seed_model': None, 'ema_update_freq': 1, 'ema_fp32': False}}\n","2024-10-21 04:57:26 | INFO | fairseq.tasks.translation | [ja] dictionary: 146832 types\n","2024-10-21 04:57:26 | INFO | fairseq.tasks.translation | [en] dictionary: 221864 types\n","2024-10-21 04:57:26 | INFO | fairseq_cli.interactive | loading model(s) from checkpoints/kftt-checkpoints/checkpoint_best.pt\n","/usr/local/lib/python3.10/dist-packages/fairseq/checkpoint_utils.py:315: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n","  state = torch.load(f, map_location=torch.device(\"cpu\"))\n","2024-10-21 04:57:42 | INFO | fairseq_cli.interactive | NOTE: hypothesis and token scores are output in base 2\n","2024-10-21 04:57:42 | INFO | fairseq_cli.interactive | Type the input sentence and press return:\n","S-0\t<unk>\n","W-0\t2.008\tseconds\n","H-0\t-2.3487508296966553\tIt is written as follows :\n","D-0\t-2.3487508296966553\tIt is written as follows :\n","P-0\t-7.5440 -1.7256 -3.1848 -0.8878 -2.4661 -0.6042 -0.0289\n","2024-10-21 04:57:45 | INFO | fairseq_cli.interactive | Total time: 19.300 seconds; translation time: 2.008\n"]}]},{"cell_type":"code","source":["# 93\n","# 検証データセット（valid）の日本語テキストを英語に翻訳\n","!fairseq-generate data-bin/kftt \\\n","    --path checkpoints/kftt-checkpoints/checkpoint_best.pt \\\n","    --batch-size 128 --beam 5 --remove-bpe \\\n","    --source-lang ja --target-lang en \\\n","    --gen-subset valid > translation_output.txt"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"NT30wlkJr8H-","executionInfo":{"status":"ok","timestamp":1729486746237,"user_tz":-540,"elapsed":49480,"user":{"displayName":"shuto u","userId":"05197020628006578565"}},"outputId":"47bbc4b9-b984-4f26-d172-1972b8379457"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["2024-10-21 04:58:18.327807: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n","2024-10-21 04:58:18.347795: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n","2024-10-21 04:58:18.353776: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","2024-10-21 04:58:18.367968: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n","To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n","2024-10-21 04:58:19.455942: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n","2024-10-21 04:58:22 | INFO | fairseq.tasks.text_to_speech | Please install tensorboardX: pip install tensorboardX\n","2024-10-21 04:58:23 | INFO | fairseq_cli.generate | {'_name': None, 'common': {'_name': None, 'no_progress_bar': False, 'log_interval': 100, 'log_format': None, 'log_file': None, 'aim_repo': None, 'aim_run_hash': None, 'tensorboard_logdir': None, 'wandb_project': None, 'azureml_logging': False, 'seed': 1, 'cpu': False, 'tpu': False, 'bf16': False, 'memory_efficient_bf16': False, 'fp16': False, 'memory_efficient_fp16': False, 'fp16_no_flatten_grads': False, 'fp16_init_scale': 128, 'fp16_scale_window': None, 'fp16_scale_tolerance': 0.0, 'on_cpu_convert_precision': False, 'min_loss_scale': 0.0001, 'threshold_loss_scale': None, 'amp': False, 'amp_batch_retries': 2, 'amp_init_scale': 128, 'amp_scale_window': None, 'user_dir': None, 'empty_cache_freq': 0, 'all_gather_list_size': 16384, 'model_parallel_size': 1, 'quantization_config_path': None, 'profile': False, 'reset_logging': False, 'suppress_crashes': False, 'use_plasma_view': False, 'plasma_path': '/tmp/plasma'}, 'common_eval': {'_name': None, 'path': 'checkpoints/kftt-checkpoints/checkpoint_best.pt', 'post_process': 'subword_nmt', 'quiet': False, 'model_overrides': '{}', 'results_path': None}, 'distributed_training': {'_name': None, 'distributed_world_size': 1, 'distributed_num_procs': 1, 'distributed_rank': 0, 'distributed_backend': 'nccl', 'distributed_init_method': None, 'distributed_port': -1, 'device_id': 0, 'distributed_no_spawn': False, 'ddp_backend': 'pytorch_ddp', 'ddp_comm_hook': 'none', 'bucket_cap_mb': 25, 'fix_batches_to_gpus': False, 'find_unused_parameters': False, 'gradient_as_bucket_view': False, 'fast_stat_sync': False, 'heartbeat_timeout': -1, 'broadcast_buffers': False, 'slowmo_momentum': None, 'slowmo_base_algorithm': 'localsgd', 'localsgd_frequency': 3, 'nprocs_per_node': 1, 'pipeline_model_parallel': False, 'pipeline_balance': None, 'pipeline_devices': None, 'pipeline_chunks': 0, 'pipeline_encoder_balance': None, 'pipeline_encoder_devices': None, 'pipeline_decoder_balance': None, 'pipeline_decoder_devices': None, 'pipeline_checkpoint': 'never', 'zero_sharding': 'none', 'fp16': False, 'memory_efficient_fp16': False, 'tpu': False, 'no_reshard_after_forward': False, 'fp32_reduce_scatter': False, 'cpu_offload': False, 'use_sharded_state': False, 'not_fsdp_flatten_parameters': False}, 'dataset': {'_name': None, 'num_workers': 1, 'skip_invalid_size_inputs_valid_test': False, 'max_tokens': None, 'batch_size': 128, 'required_batch_size_multiple': 8, 'required_seq_len_multiple': 1, 'dataset_impl': None, 'data_buffer_size': 10, 'train_subset': 'train', 'valid_subset': 'valid', 'combine_valid_subsets': None, 'ignore_unused_valid_subsets': False, 'validate_interval': 1, 'validate_interval_updates': 0, 'validate_after_updates': 0, 'fixed_validation_seed': None, 'disable_validation': False, 'max_tokens_valid': None, 'batch_size_valid': 128, 'max_valid_steps': None, 'curriculum': 0, 'gen_subset': 'valid', 'num_shards': 1, 'shard_id': 0, 'grouped_shuffling': False, 'update_epoch_batch_itr': False, 'update_ordered_indices_seed': False}, 'optimization': {'_name': None, 'max_epoch': 0, 'max_update': 0, 'stop_time_hours': 0.0, 'clip_norm': 0.0, 'sentence_avg': False, 'update_freq': [1], 'lr': [0.25], 'stop_min_lr': -1.0, 'use_bmuf': False, 'skip_remainder_batch': False}, 'checkpoint': {'_name': None, 'save_dir': 'checkpoints', 'restore_file': 'checkpoint_last.pt', 'continue_once': None, 'finetune_from_model': None, 'reset_dataloader': False, 'reset_lr_scheduler': False, 'reset_meters': False, 'reset_optimizer': False, 'optimizer_overrides': '{}', 'save_interval': 1, 'save_interval_updates': 0, 'keep_interval_updates': -1, 'keep_interval_updates_pattern': -1, 'keep_last_epochs': -1, 'keep_best_checkpoints': -1, 'no_save': False, 'no_epoch_checkpoints': False, 'no_last_checkpoints': False, 'no_save_optimizer_state': False, 'best_checkpoint_metric': 'loss', 'maximize_best_checkpoint_metric': False, 'patience': -1, 'checkpoint_suffix': '', 'checkpoint_shard_count': 1, 'load_checkpoint_on_all_dp_ranks': False, 'write_checkpoints_asynchronously': False, 'model_parallel_size': 1}, 'bmuf': {'_name': None, 'block_lr': 1.0, 'block_momentum': 0.875, 'global_sync_iter': 50, 'warmup_iterations': 500, 'use_nbm': False, 'average_sync': False, 'distributed_world_size': 1}, 'generation': {'_name': None, 'beam': 5, 'nbest': 1, 'max_len_a': 0.0, 'max_len_b': 200, 'min_len': 1, 'match_source_len': False, 'unnormalized': False, 'no_early_stop': False, 'no_beamable_mm': False, 'lenpen': 1.0, 'unkpen': 0.0, 'replace_unk': None, 'sacrebleu': False, 'score_reference': False, 'prefix_size': 0, 'no_repeat_ngram_size': 0, 'sampling': False, 'sampling_topk': -1, 'sampling_topp': -1.0, 'constraints': None, 'temperature': 1.0, 'diverse_beam_groups': -1, 'diverse_beam_strength': 0.5, 'diversity_rate': -1.0, 'print_alignment': None, 'print_step': False, 'lm_path': None, 'lm_weight': 0.0, 'iter_decode_eos_penalty': 0.0, 'iter_decode_max_iter': 10, 'iter_decode_force_max_iter': False, 'iter_decode_with_beam': 1, 'iter_decode_with_external_reranker': False, 'retain_iter_history': False, 'retain_dropout': False, 'retain_dropout_modules': None, 'decoding_format': None, 'no_seed_provided': False, 'eos_token': None}, 'eval_lm': {'_name': None, 'output_word_probs': False, 'output_word_stats': False, 'context_window': 0, 'softmax_batch': 9223372036854775807}, 'interactive': {'_name': None, 'buffer_size': 0, 'input': '-'}, 'model': {'_name': 'wav2vec2', 'extractor_mode': 'default', 'encoder_layers': 12, 'encoder_embed_dim': 768, 'encoder_ffn_embed_dim': 3072, 'encoder_attention_heads': 12, 'activation_fn': 'gelu', 'layer_type': 'transformer', 'dropout': 0.1, 'attention_dropout': 0.1, 'activation_dropout': 0.0, 'encoder_layerdrop': 0.0, 'dropout_input': 0.0, 'dropout_features': 0.0, 'final_dim': 0, 'layer_norm_first': False, 'conv_feature_layers': '[(512, 10, 5)] + [(512, 3, 2)] * 4 + [(512,2,2)] + [(512,2,2)]', 'conv_bias': False, 'logit_temp': 0.1, 'quantize_targets': False, 'quantize_input': False, 'same_quantizer': False, 'target_glu': False, 'feature_grad_mult': 1.0, 'quantizer_depth': 1, 'quantizer_factor': 3, 'latent_vars': 320, 'latent_groups': 2, 'latent_dim': 0, 'mask_length': 10, 'mask_prob': 0.65, 'mask_selection': 'static', 'mask_other': 0.0, 'no_mask_overlap': False, 'mask_min_space': 1, 'require_same_masks': True, 'mask_dropout': 0.0, 'mask_channel_length': 10, 'mask_channel_prob': 0.0, 'mask_channel_before': False, 'mask_channel_selection': 'static', 'mask_channel_other': 0.0, 'no_mask_channel_overlap': False, 'mask_channel_min_space': 1, 'num_negatives': 100, 'negatives_from_everywhere': False, 'cross_sample_negatives': 0, 'codebook_negatives': 0, 'conv_pos': 128, 'conv_pos_groups': 16, 'pos_conv_depth': 1, 'latent_temp': [2.0, 0.5, 0.999995], 'max_positions': 100000, 'checkpoint_activations': False, 'required_seq_len_multiple': 1, 'crop_seq_to_multiple': 1, 'depthwise_conv_kernel_size': 31, 'attn_type': '', 'pos_enc_type': 'abs', 'fp16': False}, 'task': {'_name': 'translation', 'data': 'data-bin/kftt', 'source_lang': 'ja', 'target_lang': 'en', 'load_alignments': False, 'left_pad_source': True, 'left_pad_target': False, 'max_source_positions': 1024, 'max_target_positions': 1024, 'upsample_primary': -1, 'truncate_source': False, 'num_batch_buckets': 0, 'train_subset': 'train', 'dataset_impl': None, 'required_seq_len_multiple': 1, 'eval_bleu': False, 'eval_bleu_args': '{}', 'eval_bleu_detok': 'space', 'eval_bleu_detok_args': '{}', 'eval_tokenized_bleu': False, 'eval_bleu_remove_bpe': None, 'eval_bleu_print_samples': False}, 'criterion': {'_name': 'cross_entropy', 'sentence_avg': True}, 'optimizer': None, 'lr_scheduler': {'_name': 'fixed', 'force_anneal': None, 'lr_shrink': 0.1, 'warmup_updates': 0, 'lr': [0.25]}, 'scoring': {'_name': 'bleu', 'pad': 1, 'eos': 2, 'unk': 3}, 'bpe': None, 'tokenizer': None, 'ema': {'_name': None, 'store_ema': False, 'ema_decay': 0.9999, 'ema_start_update': 0, 'ema_seed_model': None, 'ema_update_freq': 1, 'ema_fp32': False}}\n","2024-10-21 04:58:24 | INFO | fairseq.tasks.translation | [ja] dictionary: 146832 types\n","2024-10-21 04:58:24 | INFO | fairseq.tasks.translation | [en] dictionary: 221864 types\n","2024-10-21 04:58:24 | INFO | fairseq_cli.generate | loading model(s) from checkpoints/kftt-checkpoints/checkpoint_best.pt\n","/usr/local/lib/python3.10/dist-packages/fairseq/checkpoint_utils.py:315: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n","  state = torch.load(f, map_location=torch.device(\"cpu\"))\n","2024-10-21 04:58:29 | INFO | fairseq.data.data_utils | loaded 1,166 examples from: data-bin/kftt/valid.ja-en.ja\n","2024-10-21 04:58:29 | INFO | fairseq.data.data_utils | loaded 1,166 examples from: data-bin/kftt/valid.ja-en.en\n","2024-10-21 04:58:29 | INFO | fairseq.tasks.translation | data-bin/kftt valid ja-en 1166 examples\n","2024-10-21 04:58:58 | INFO | fairseq_cli.generate | NOTE: hypothesis and token scores are output in base 2\n","2024-10-21 04:58:58 | INFO | fairseq_cli.generate | Translated 1,166 sentences (27,076 tokens) in 22.8s (51.17 sentences/s, 1188.22 tokens/s)\n"]}]},{"cell_type":"code","source":["#BLEUスコアの計測\n","!tail -n 1 translation_output.txt | grep \"BLEU\""],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"p5Ivc-oqr9dQ","executionInfo":{"status":"ok","timestamp":1729486746237,"user_tz":-540,"elapsed":5,"user":{"displayName":"shuto u","userId":"05197020628006578565"}},"outputId":"14b70888-11d3-45b7-b11f-4146ca449652"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Generate valid with beam=5: BLEU4 = 16.07, 47.3/21.0/11.0/6.1 (BP=1.000, ratio=1.066, syslen=25910, reflen=24309)\n"]}]},{"cell_type":"code","source":["#95,96\n","!pip install subword-nmt\n","!pip install tensorboard tensorboardX\n","\n","# サブワードの学習\n","!subword-nmt learn-bpe -s 16000 < kftt-data-1.0/data/tok/kyoto-train.ja > ja.bpe\n","!subword-nmt learn-bpe -s 16000 < kftt-data-1.0/data/tok/kyoto-train.en > en.bpe\n","\n","# 学習したBPEルールを使ってデータをサブワード分割\n","!subword-nmt apply-bpe -c ja.bpe < kftt-data-1.0/data/tok/kyoto-train.ja > kftt-data-1.0/data/tok/kyoto-train.bpe.ja\n","!subword-nmt apply-bpe -c en.bpe < kftt-data-1.0/data/tok/kyoto-train.en > kftt-data-1.0/data/tok/kyoto-train.bpe.en\n","!subword-nmt apply-bpe -c ja.bpe < kftt-data-1.0/data/tok/kyoto-dev.ja > kftt-data-1.0/data/tok/kyoto-dev.bpe.ja\n","!subword-nmt apply-bpe -c en.bpe < kftt-data-1.0/data/tok/kyoto-dev.en > kftt-data-1.0/data/tok/kyoto-dev.bpe.en\n","!subword-nmt apply-bpe -c ja.bpe < kftt-data-1.0/data/tok/kyoto-test.ja > kftt-data-1.0/data/tok/kyoto-test.bpe.ja\n","!subword-nmt apply-bpe -c en.bpe < kftt-data-1.0/data/tok/kyoto-test.en > kftt-data-1.0/data/tok/kyoto-test.bpe.en"],"metadata":{"id":"4bLl70O7sLQX"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["!fairseq-preprocess \\\n","    --source-lang ja --target-lang en \\\n","    --trainpref kftt-data-1.0/data/tok/kyoto-train.bpe \\\n","    --validpref kftt-data-1.0/data/tok/kyoto-dev.bpe \\\n","    --testpref kftt-data-1.0/data/tok/kyoto-test.bpe \\\n","    --destdir data-bin/kftt-bpe \\\n","    --workers 2"],"metadata":{"id":"9Zs0HIn01T0g"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["!fairseq-train data-bin/kftt-bpe \\\n","    --arch lstm --share-decoder-input-output-embed \\\n","    --optimizer adam --lr 0.001 --clip-norm 0.1 \\\n","    --lr-scheduler inverse_sqrt --warmup-updates 4000 \\\n","    --dropout 0.3 --max-tokens 4096 --fp16 \\\n","    --save-dir checkpoints/kftt-bpe-checkpoints \\\n","    --max-epoch 7"],"metadata":{"id":"u1RiB-sZ1bTo"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["!fairseq-generate data-bin/kftt-bpe \\\n","    --path checkpoints/kftt-bpe-checkpoints/checkpoint_best.pt \\\n","    --batch-size 128 --beam 5 --remove-bpe \\\n","    --source-lang ja --target-lang en \\\n","    --gen-subset test > translation_output_bpe.txt\n","\n","# BLEUスコアの取得\n","!tail -n 10 translation_output_bpe.txt | grep \"BLEU\""],"metadata":{"id":"qs1P3n961fBk"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["--tensorboard-logdir logs/tensorboard"],"metadata":{"id":"Ul4fGkyH3zy8"},"execution_count":null,"outputs":[]}]}